# CTC Decoding Algorithms with Language Model

## Algorithms
- Best Path Decoding: takes best labels per time-step and then removes repeated labels and blanks. Function: ctcBestPath. \[1]
- Beam Search Decoding: iteratively searches for best labelling, uses a character bigram language model. Function: ctcBeamSearch. \[2]
- Token Passing: searches for most probable word sequence, words are restricted to the words from a dictionary. Can be extended to also use a word-bigram language model. Function: ctcTokenPassing. \[1]

## Run
```python ctcDecoder.py```

Expected results:
```
=====Mini example=====
BEST PATH  : ""
BEAM SEARCH: "a"
=====Real example=====
BEST PATH  : "the fak friend of the fomly hae tC"
BEAM SEARCH: "the fake friend of the family, lie th"
TOKEN      : "the fake friend of the family fake the"
```

The ground truth text is "the fake friend of the family, like the" and is one sample from the IAM Handwriting Database. The used RNN output was generated by a partially trained TensorFlow model.

## Files
- ctcDecoder.py: contains the three decoding algorithms
- data/rnnOutput.csv: output of RNN layer (softmax not yet applied), which contains 100 time-steps and 80 label scores per time-step
- data/corpus.txt: the text from which the language model is generated, which in this case is just the ground truth text

## Note
This python script is useful for experiments. For productive use I implemented the functions in C++ and then added them to TensorFlow as a costum op to achieve good performance. 


\[1] Graves - Supervised sequence labelling with recurrent neural networks

\[2] Hwang - Character-level incremental speech recognition with recurrent neural networks
